<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Challenges in Evaluating LLMs: A Statistical Analysis of Chatbot Arena Leaderboard | Mohammad Arvan</title> <meta name="author" content="Mohammad Arvan"> <meta name="description" content="Comparing GPT-4 and Claude-v1 using statistical analysis"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%93%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mo-arvan.github.io/blog/2024/chatbot-arena/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Challenges in Evaluating LLMs: A Statistical Analysis of Chatbot Arena Leaderboard",
      "description": "Comparing GPT-4 and Claude-v1 using statistical analysis",
      "published": "August 13, 2024",
      "authors": [
        {
          "author": "Mohammad Arvan",
          "authorURL": "https://mo-arvan.github.io/",
          "affiliations": [
            {
              "name": "Department of Computer Science, University of Illinois at Chicago",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mohammad </span>Arvan</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/personal/">personal</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item"> <a class="nav-link" href="https://mo-arvan.github.io/assets/pdf/Mohammad_Arvan_CV.pdf"> CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Challenges in Evaluating LLMs: A Statistical Analysis of Chatbot Arena Leaderboard</h1> <p>Comparing GPT-4 and Claude-v1 using statistical analysis</p> </d-title><d-byline></d-byline><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#abstract">Abstract</a></div> <div><a href="#statistical-analysis-of-chatbot-arena-leaderboard">Statistical Analysis of Chatbot Arena Leaderboard</a></div> </nav> </d-contents> <h2 id="abstract">Abstract</h2> <p>Evaluating LLMs, particularly within the Chatbot Arena leaderboard, poses substantial challenges due to the variability in model outputs and the dependence on human judgment. In this post, I employ statistical analysis to compare GPT-4 and Claude-v1. My findings indicate that although differences between models is statistically significant, the reliability of these results is compromised by insufficient sample sizes. Ultimately, this analysis showcases the application of statistical methods in evaluating LLMs and highlights the need for more rigorous evaluation practices in the field.</p> <h2 id="statistical-analysis-of-chatbot-arena-leaderboard">Statistical Analysis of Chatbot Arena Leaderboard</h2> <p>Progress in machine learning research is often driven by benchmarks as an objective method for comparing different models and algorithms. One prominent benchmark in the field of conversational AI is the Chatbot Arena, a leaderboard that ranks chatbot models based on ratings from human evaluators. However, evaluating generated text remains a complex challenge. In this post, I will discuss these challenges and present a statistical approach to investigate the reproducibility of the Chatbot Arena leaderboard.</p> <p>Evaluating generated text is particularly difficult due to several factors. First, the model’s output can vary because text generation involves sampling from a distribution. Moreover, there may be multiple correct answers to a given prompt. Additionally, while the goal is objective evaluation, subjective human judgment still plays a significant role, whether through annotated data for automatic benchmarks or direct human preference of model performance.</p> <p>Despite these challenges, benchmarks remain essential for comparing models. The next question is: how can we increase trust in these benchmarks? One of the most critical properties of a benchmark is reproducibility. Ideally, repeating the same experiment should yield similar results. For instance, if I conduct an evaluation and find that model A outperforms model B, another researcher should observe the same result under identical conditions. However, recent research suggests that reproducibility is not guaranteed; specific design decisions and experimental aspects can lead to high variability in results.</p> <p>Consider these two reproducibility studies—<a href="https://aclanthology.org/2023.humeval-1.7.pdf" rel="external nofollow noopener" target="_blank">Van Miltenburg et al.</a> and <a href="https://aclanthology.org/2023.humeval-1.8.pdf" rel="external nofollow noopener" target="_blank">Arvan et al.</a>—which evaluated an ML pipeline (or see Chapter 4 of my <a href="https://mo-arvan.github.io/assets/pdf/papers/Mohammad_Arvan_Thesis.pdf">dissertation</a>). Despite following the same process, the ranking flipped between the two evaluations. This phenomenon is not unique; over 10 other reproducibility studies from the ReproHum project—an initiative for testing the reproducibility of human evaluation in ML and NLP—have found similar issues (<a href="https://aclanthology.org/2023.humeval-1.4.pdf" rel="external nofollow noopener" target="_blank">Belz et al., 2023</a> and <a href="https://aclanthology.org/2024.humeval-1.9.pdf" rel="external nofollow noopener" target="_blank">Belz et al., 2024</a>).</p> <p>Many factors contribute to the reproducibility (or lack there of) evaluations. Although the ReproHum project is ongoing, evidence suggests that statistical analysis can help understand variability in human evaluations. Significance testing, a standard practice in many scientific fields is a common tool to determine if the difference between two groups is due to random chance or a real difference. Unfortunately, in Computer Science, significance testing is underutilized.</p> <p>A common test is the t-test, which can evaluate whether the difference in mean preference scores between two models is statistically significant. It returns a <code class="language-plaintext highlighter-rouge">p-value</code>, the probability of observing the current data or data that is more extreme if the null hypothesis is true. If the <code class="language-plaintext highlighter-rouge">p-value</code> is less than a significance level (e.g., 0.05), we reject the null hypothesis and conclude that there is a significant difference between the two models. Otherwise, we fail to reject the null hypothesis indicating insufficient evidence to claim a difference.</p> <p>However, significance testing also involves other important details: effect size and power analysis. Effect size measures the magnitude of the difference between two groups, while power analysis determines the sample size needed to detect a difference. For instance, a power of 0.8 means there’s an 80% chance of detecting a difference if one exists. The significance level (alpha) is typically set at 0.05, indicating a 5% chance of detecting a difference that doesn’t exist. As a reference, <a href="https://www.taylorfrancis.com/books/mono/10.4324/9780203771587/statistical-power-analysis-behavioral-sciences-jacob-cohen" rel="external nofollow noopener" target="_blank">Cohen</a> suggests that a small effect size is 0.2, medium is 0.5, and large is 0.8.</p> <p>While Chatbot Arena provides a leaderboard, pairwise comparisons of models more closely reflect practical model selection. For example, if I want to compare a production model (Model A) with a new model (Model B), I need to determine whether the performance gains justify transitioning to Model B. If the gains are marginal, I might stick with Model A. This is essentially A/B testing in the context of model evaluation.</p> <p>To apply this analysis to the Chatbot Arena leaderboard, I utilized a <a href="https://huggingface.co/datasets/lmsys/chatbot_arena_conversations" rel="external nofollow noopener" target="_blank">dataset</a> released by LMSYS, the organization behind Chatbot Arena. Although the dataset is almost a year old, it remains a valuable resource. I plan to request an updated dataset, but for now, I will use the existing data to demonstrate the analysis.</p> <p>Focusing on the first- and second-place models—GPT-4 and Claude-v1—there are 321 pairwise comparisons between them. To detect an effect size of 0.8, we need 26 comparisons; for effect sizes of 0.5 and 0.2, we need 64 and 394 comparisons, respectively. Conceptually, this means the closer the performance of two models, the larger the sample size needed to detect a difference.</p> <p>In the current setting, if the empirical effect size is 0.5 or greater, the sample size is sufficient. However, if the effect size is less than 0.5, the significance test is at higher risk of false positives.</p> <p>Out of the 321 comparisons, GPT-4 won 37% of the time, Claude-v1 won 30%, and 33% were ties. I calculate a score array for each model: 1 point for a win, -1 for a loss, and 0 for a tie. Ideally, each comparison would involve multiple human evaluators, but in this setup, only one evaluator was used per comparison—one of the limitations of the current Chatbot Arena structure.</p> <p>Our null hypothesis is that there is no difference in scores of GPT-4 and Claude-v1.</p> <p>A t-test on the scores of GPT-4 and Claude-v1 yielded a p-value of 0.0265, a test statistic of 2.2, and an empirical effect size of 0.18. While the difference between the two models is statistically significant, the effect size is less than 0.2, indicating the test may be underpowered. Thus, there is an elevated risk of false positives. In other words, we cannot confidently conclude that GPT-4 is superior to Claude-v1, and drawing strong conclusions may lead to future reproducibility issues.</p> <p>This analysis can be extended to other models on the leaderboard. For instance, we could compare models within the same API cost tier, providing a more nuanced comparison. Moving beyond a single score to a more comprehensive analysis is a step in the right direction. Rigorous evaluations are crucial in the landscape of LLMs.</p> <p>I look forward to obtaining the updated dataset from LMSYS to conduct a more thorough analysis. With the additional data and inclusion of the latest models, some of the concerns raised in this post may be addressed. I welcome your thoughts on this analysis and any suggestions for future work. The source code for this analysis is available on my <a href="https://github.com/mo-arvan/chatbot-arena-analysis" rel="external nofollow noopener" target="_blank">GitHub repository</a>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Mohammad Arvan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: August 13, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>