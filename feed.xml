<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://mo-arvan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mo-arvan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-13T19:34:09+00:00</updated><id>https://mo-arvan.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://mo-arvan.github.io/blog/2024/2024-08-13-chatbot-arena/" rel="alternate" type="text/html" title=""/><published>2024-08-13T19:34:09+00:00</published><updated>2024-08-13T19:34:09+00:00</updated><id>https://mo-arvan.github.io/blog/2024/2024-08-13-chatbot-arena</id><content type="html" xml:base="https://mo-arvan.github.io/blog/2024/2024-08-13-chatbot-arena/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>Evaluating LLMs, particularly within the Chatbot Arena leaderboard, poses substantial challenges due to the variability in model outputs and the dependence on human judgment. In this post, I employ statistical analysis to assess the reproducibility of leaderboard rankings, with a focus on comparing GPT-4 and Claude-v1. My findings indicate that although differences between models may seem statistically significant, the reliability of these results is often compromised by insufficient sample sizes. I recommend more rigorous evaluation methods and present practical approach for model selection in conversational AI.</p> <h2 id="challenges-in-evaluating-llms-a-statistical-analysis-of-chatbot-arena-leaderboard">Challenges in Evaluating LLMs: A Statistical Analysis of Chatbot Arena Leaderboard</h2> <p>Progress in machine learning research is often driven by benchmarks, which provide objective comparisons between different models and algorithms. One prominent benchmark in the field of conversational AI is the Chatbot Arena, a leaderboard that ranks chatbot models based on ratings from human evaluators. However, evaluating generated text remains a complex challenge. In this post, I will discuss these challenges and present a statistical approach to investigate the reproducibility of the Chatbot Arena leaderboard.</p> <p>Evaluating generated text is particularly difficult due to several factors. First, the model’s output can vary because text generation involves sampling from a distribution. Second, there may be multiple correct answers to a given prompt. Lastly, while the goal is objective evaluation, human judgment still plays a significant role, whether through annotated data for automatic benchmarks or direct human comparisons of model performance.</p> <p>Despite these challenges, benchmarks are essential for comparing models. The next question is: how can we increase trust in these benchmarks? One of the most critical properties of a benchmark is reproducibility. Ideally, repeating the same experiment should yield similar results. For instance, if I conduct an evaluation and find that model A outperforms model B, another researcher should observe the same result under identical conditions. However, recent research suggests that reproducibility is not guaranteed; specific design decisions and experimental aspects can lead to high variability in results.</p> <p>Consider these two reproducibility studies—<a href="https://aclanthology.org/2023.humeval-1.7.pdf">Van Miltenburg et al.</a> and <a href="https://aclanthology.org/2023.humeval-1.8.pdf">Arvan et al.</a>—which evaluated an ML pipeline (or see Chapter 4 of my <a href="https://mo-arvan.github.io/assets/pdf/papers/Mohammad_Arvan_Thesis.pdf">dissertation</a>). Despite following the same process, the ranking flipped between the two evaluations. This phenomenon is not unique; over 10 other reproducibility studies from the ReproHum project—an initiative for testing the reproducibility of human evaluation in ML and NLP—have found similar issues (<a href="https://aclanthology.org/2023.humeval-1.4.pdf">Belz et al., 2023</a> and <a href="https://aclanthology.org/2024.humeval-1.9.pdf">Belz et al., 2024</a>).</p> <p>Many factors contribute to the reproducibility (or lack there of) of evaluations. Although the ReproHum project is ongoing, evidence suggests that statistical analysis can help understand variability in human evaluations. Significance testing, a standard practice in many scientific fields is a common tool to determine if the difference between two groups is due to random chance or a real difference. Unfortunately, in Computer Science, significance testing is underutilized.</p> <p>A common test is the t-test, which can evaluate whether the difference in mean preference scores (wins) between two models is statistically significant. However, significance testing also involves other important aspects: effect size and power analysis. Effect size measures the magnitude of the difference between two groups, while power analysis determines the sample size needed to detect a difference. For instance, a power of 0.8 means there’s an 80% chance of detecting a difference if one exists. The significance level (alpha) is typically set at 0.05, indicating a 5% chance of detecting a difference that doesn’t exist. As a reference, Cohen suggests that a small effect size is 0.2, medium is 0.5, and large is 0.8.</p> <p>While Chatbot Arena provides a leaderboard, pairwise comparisons of models more closely reflect practical model selection. For example, if I want to compare a production model (Model A) with a new model (Model B), I need to determine whether the performance gains justify transitioning to Model B. If the gains are marginal, I might stick with Model A. This is essentially A/B testing in the context of model evaluation.</p> <p>To apply this analysis to the Chatbot Arena leaderboard, I utilized a <a href="https://huggingface.co/datasets/lmsys/chatbot_arena_conversations">dataset</a> released by LMSYS, the organization behind Chatbot Arena. Although the dataset is almost a year old, it remains a valuable resource. I plan to request an updated dataset, but for now, I will use the existing data to demonstrate the analysis.</p> <p>Focusing on the first- and second-place models—GPT-4 and Claude-v1—there are 321 pairwise comparisons between them. To detect an effect size of 0.8, we need 26 comparisons; for effect sizes of 0.5 and 0.2, we need 64 and 394 comparisons, respectively. Conceptually, this means the closer the performance of two models, the larger the sample size needed to detect a difference.</p> <p>In the current setting, if the empirical effect size is 0.5 or greater, the sample size is sufficient. However, if the effect size is less than 0.5, the significance test is at higher risk of false positives.</p> <p>Out of the 321 comparisons, GPT-4 won 37% of the time, Claude-v1 won 30%, and 33% were ties. We calculated a score array for each model: 1 point for a win, -1 for a loss, and 0 for a tie. Ideally, each comparison would involve multiple human evaluators, but in this setup, only one evaluator was used per comparison—one of the limitations of the current Chatbot Arena structure.</p> <p>A t-test on the scores of GPT-4 and Claude-v1 yielded a p-value of 0.0265, a test statistic of 2.2, and an empirical effect size of 0.18. While the difference between the two models is statistically significant, the effect size is less than 0.2, indicating the test may be underpowered. Thus, we cannot confidently conclude that GPT-4 is superior to Claude-v1, and drawing strong conclusions may lead to future reproducibility issues.</p> <p>This analysis can be extended to other models on the leaderboard. For instance, we could compare models within the same API cost tier, providing a more nuanced comparison. Moving beyond a single score to a more comprehensive analysis is a step in the right direction. Rigorous evaluations are crucial in the landscape of LLMs.</p> <p>I look forward to obtaining the updated dataset from LMSYS to conduct a more thorough analysis. With the additional data and inclusion of the latest models, some of the concerns raised in this post may be addressed. I welcome your thoughts on this analysis and any suggestions for future work. The source code for this analysis is available on my <a href="https://github.com/mo-arvan/chatbot-arena-analysis">GitHub repository</a>.</p>]]></content><author><name></name></author></entry><entry><title type="html">On OpenLLM Leaderboard</title><link href="https://mo-arvan.github.io/blog/2024/on-openllm-leaderboard/" rel="alternate" type="text/html" title="On OpenLLM Leaderboard"/><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://mo-arvan.github.io/blog/2024/on-openllm-leaderboard</id><content type="html" xml:base="https://mo-arvan.github.io/blog/2024/on-openllm-leaderboard/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The OpenLLM leaderboard is a collection of benchmarks for large language models. Unlike numbers reported in marketing materials, the results on the leaderboard are reproducible and can be verified by anyone. This leaderboard is managed by the Hugging Face team and is based on the Harness (lm-eval) package from EleutherAI. Overall, the leaderboard is a valuable resource for the community to compare different models and track progress in the field. Recently, the Hugging Face team announced several changes to the leaderboard. In this post, I will review these changes and discuss their implications.</p> <h2 id="summary-of-changes">Summary of Changes</h2> <p>In their blog post, the Hugging Face team raise several issues with this leaderboard. Firstly, they highlight the problem of saturation, where performance on certain benchmarks reaches human level.</p> <p>Moreover, they suggest that certain models are showing signs of data contamination. This is a serious issue that can lead to misleading results and conclusions. Without a clear chain of custody for the data used in training, it’s difficult to confirm data contamination in a model.</p> <p>Finally, they point out some of the recent investigations into issues in datasets quality, particularly MMLU-Redux and MMLU-Pro.</p> <p>To address these issues, they are rebooting the leaderboard with newly released benchmarks and high-quality datasets. Additionally, they are introducing a normalization step, setting a random baseline as 0 and the maximum score as 100. They also updated the Harness (lm-eval) package from EleutherAI to its latest version and added support for LoRA finetuning. They claim these changes should enhance the reproducibility of the results.</p> <p>To combat data contamination in submitted models, they are introducing a maintainer’s highlight. They will handpick models released by reputable organizations that have been thoroughly vetted. This list will remain open and evolve based on community feedback and their own observations.</p> <p>Lastly, they are introducing a voting mechanism for running new models on the leaderboard. This aims to address the issue of submission spamming, where multiple variants of a model are submitted to see which performs best. They hope the voting mechanism will mitigate this problem. Additionally, there are user interface improvements thanks to the work of the Gradio team.</p> <h2 id="technical-review">Technical Review</h2> <p>I find all these changes to be positive steps towards more reliable and reproducible benchmarks. While the maintainer’s highlight section could introduce biases from the Hugging Face team, their language suggests they are open to feedback and aim to be community-driven. Given the challenges of identifying models trained on contaminated data, I believe this is a reasonable compromise. However, I still think the evaluation process could be improved.</p> <p>My primary concern with this leaderboard is the lack of effort to account for stochasticity in the evaluation process. Although training neural networks involves sources of randomness such as weight initialization, data order, and dropout, once trained, these models become deterministic. They output a probability distribution for each class they are trained on, and in the case of language models, the classes are the tokens in the vocabulary. Greedy selection of the highest probability results in generated text that often appears “robotic.” To address this, the common approach is to use sampling with an adjusted temperature for the softmax function, introducing randomness in the generation process. Given that two datasets, IFEval and MATH, utilize text generation, it is important to account for this variability and report a set of results for each model.</p> <p>Regarding other tasks, it seems that lm-harness is moving towards a standardized template and format for all model evaluations. While this is systematic, it might backfire because in real-world use cases, inputs might not always be in the same format. This could result in models that are not robust to different inputs. Therefore, I believe evaluating the models with various input formats and reporting a set of results would provide a more accurate picture of the models’ performance.</p> <p>Both of these changes would indeed necessitate additional computational resources and time. It’s possible that lm-eval already addresses such variability, although the blog post does not mention it explicitly. In my next post, I will delve deeper into the source code of lm-eval and see if these concerns are already addressed.</p>]]></content><author><name>Mohammad Arvan</name></author><category term="ML"/><category term="Evaluation"/><category term="LLM"/><category term="NLP"/><summary type="html"><![CDATA[Technical review of the latest changes in the OpenLLM leaderboard]]></summary></entry></feed>